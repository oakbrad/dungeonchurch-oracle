name: Data Visualization Pipeline

on:
  schedule:
    # Run once a day at 430PST (1130 UTC)
    - cron: '30 11 * * *'
  workflow_dispatch:
    # Allow manual triggering
permissions:                  # 
  contents: write             # gives write access to the repo

jobs:
  process-data:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_USER: postgres
          POSTGRES_DB: postgres
        ports:
          - 5432:5432
        # Set health checks to wait until postgres has started
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          # Fetch all history for all branches and tags
          fetch-depth: 0
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install psycopg2-binary requests
          sudo apt-get update
          sudo apt-get install -y postgresql-client
      
      - name: Download latest database dump
        id: download
        env:
          DUNGEONCHURCH_S3_URL: ${{ secrets.DUNGEONCHURCH_S3_URL }}
          DUNGEONCHURCH_S3_NAMESPACE: ${{ secrets.DUNGEONCHURCH_S3_NAMESPACE }}
          DUNGEONCHURCH_S3_BUCKET: ${{ secrets.DUNGEONCHURCH_S3_BUCKET }}
        run: |
          mkdir -p data
          python scripts/download_latest_dump.py
          # Check if download was successful by looking for dump files
          if ls data/*.dump 1> /dev/null 2>&1; then
            echo "Download successful"
            echo "DOWNLOAD_SUCCESS=true" >> $GITHUB_OUTPUT
            # Get the path of the downloaded file
            DUMP_FILE=$(ls -t data/*.dump | head -1)
            echo "DUMP_FILE=$DUMP_FILE" >> $GITHUB_OUTPUT
          else
            echo "Download failed"
            echo "DOWNLOAD_SUCCESS=false" >> $GITHUB_OUTPUT
            exit 1
          fi
      
      - name: Process relationships data
        id: process
        if: steps.download.outputs.DOWNLOAD_SUCCESS == 'true'
        env:
          POSTGRES_HOST: localhost
          POSTGRES_PORT: 5432
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          EXCLUDE_5E: ${{ secrets.EXCLUDE_5E }}
        run: |
          # Get the path of the downloaded file
          DUMP_FILE="${{ steps.download.outputs.DUMP_FILE }}"
          echo "Processing dump file: $DUMP_FILE"
          python scripts/run_pipeline.py --output data/graph_data.json --dump-file "$DUMP_FILE"
      
      - name: Commit and push if data changed
        if: steps.download.outputs.DOWNLOAD_SUCCESS == 'true' && success()
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          
          # Check if graph_data.json or orphan_data.json exist and have changes
          CHANGES_DETECTED=false
          
          if [ -f data/graph_data.json ]; then
            git add data/graph_data.json
            
            # Check if there are changes to graph_data.json
            if ! git diff --staged --quiet; then
              CHANGES_DETECTED=true
              echo "Changes detected in graph_data.json"
            else
              echo "No changes to graph_data.json"
            fi
          else
            echo "graph_data.json not found"
            exit 1
          fi
          
          # Check for orphan_data.json
          if [ -f data/orphan_data.json ]; then
            git add data/orphan_data.json
            
            # Check if there are changes to orphan_data.json
            if ! git diff --staged --quiet; then
              CHANGES_DETECTED=true
              echo "Changes detected in orphan_data.json"
            else
              echo "No changes to orphan_data.json"
            fi
          else
            echo "orphan_data.json not found"
          fi
          
          # Check for collection color files
          if [ -f static/css/collection-colors.css ]; then
            git add static/css/collection-colors.css
            
            # Check if there are changes to collection-colors.css
            if ! git diff --staged --quiet; then
              CHANGES_DETECTED=true
              echo "Changes detected in collection-colors.css"
            else
              echo "No changes to collection-colors.css"
            fi
          else
            echo "collection-colors.css not found"
          fi
          
          if [ -f static/js/collection-colors.js ]; then
            git add static/js/collection-colors.js
            
            # Check if there are changes to collection-colors.js
            if ! git diff --staged --quiet; then
              CHANGES_DETECTED=true
              echo "Changes detected in collection-colors.js"
            else
              echo "No changes to collection-colors.js"
            fi
          else
            echo "collection-colors.js not found"
          fi
          
          # Commit and push if any changes were detected
          if [ "$CHANGES_DETECTED" = true ]; then
            git commit -m "Update graph data and collection colors"
            git push
            echo "Updated data files committed and pushed"
          else
            echo "No changes detected in any data files"
          fi
      
      - name: Clean up dump file
        if: always()
        run: |
          # Remove any dump files to ensure sensitive data is not kept
          rm -f data/*.dump
          echo "Cleaned up dump files"
