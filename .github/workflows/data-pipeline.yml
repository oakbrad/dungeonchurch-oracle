name: Data Visualization Pipeline

on:
  schedule:
    # Run once a day at midnight UTC
    - cron: '0 0 * * *'
  workflow_dispatch:
    # Allow manual triggering

jobs:
  process-data:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_USER: postgres
          POSTGRES_DB: postgres
        ports:
          - 5432:5432
        # Set health checks to wait until postgres has started
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          # Fetch all history for all branches and tags
          fetch-depth: 0
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install psycopg2-binary requests
          sudo apt-get update
          sudo apt-get install -y postgresql-client
      
      - name: Download latest database dump
        id: download
        env:
          DUNGEONCHURCH_S3_URL: ${{ secrets.DUNGEONCHURCH_S3_URL }}
          DUNGEONCHURCH_S3_NAMESPACE: ${{ secrets.DUNGEONCHURCH_S3_NAMESPACE }}
          DUNGEONCHURCH_S3_BUCKET: ${{ secrets.DUNGEONCHURCH_S3_BUCKET }}
        run: |
          mkdir -p data
          python scripts/download_latest_dump.py
          # Check if download was successful by looking for dump files
          if ls data/*.dump 1> /dev/null 2>&1; then
            echo "Download successful"
            echo "DOWNLOAD_SUCCESS=true" >> $GITHUB_OUTPUT
            # Get the path of the downloaded file
            DUMP_FILE=$(ls -t data/*.dump | head -1)
            echo "DUMP_FILE=$DUMP_FILE" >> $GITHUB_OUTPUT
          else
            echo "Download failed"
            echo "DOWNLOAD_SUCCESS=false" >> $GITHUB_OUTPUT
            exit 1
          fi
      
      - name: Modify process_relationships.py to use environment variables
        if: steps.download.outputs.DOWNLOAD_SUCCESS == 'true'
        run: |
          # Create a temporary script to modify the process_relationships.py file
          cat > modify_script.py << 'EOF'
          import re
          
          # Read the original file
          with open('scripts/process_relationships.py', 'r') as f:
              content = f.read()
          
          # Modify the database connection to use environment variables
          content = re.sub(
              r'conn = psycopg2\.connect\(f"dbname=\{temp_db_name\}"\)',
              'conn = psycopg2.connect(\n            host=os.environ.get("POSTGRES_HOST", "localhost"),\n            port=os.environ.get("POSTGRES_PORT", "5432"),\n            user=os.environ.get("POSTGRES_USER", "postgres"),\n            password=os.environ.get("POSTGRES_PASSWORD", "postgres"),\n            dbname=temp_db_name\n        )',
              content
          )
          
          # Modify the database creation command to use environment variables
          content = re.sub(
              r'subprocess\.run\(\["createdb", temp_db_name\], check=True\)',
              'subprocess.run([\n            "createdb",\n            "-h", os.environ.get("POSTGRES_HOST", "localhost"),\n            "-p", os.environ.get("POSTGRES_PORT", "5432"),\n            "-U", os.environ.get("POSTGRES_USER", "postgres"),\n            temp_db_name\n        ], check=True, env={**os.environ, "PGPASSWORD": os.environ.get("POSTGRES_PASSWORD", "postgres")})',
              content
          )
          
          # Modify the database restoration command to use environment variables
          content = re.sub(
              r'subprocess\.run\(\["pg_restore", "-d", temp_db_name, dump_file\],',
              'subprocess.run([\n            "pg_restore",\n            "-h", os.environ.get("POSTGRES_HOST", "localhost"),\n            "-p", os.environ.get("POSTGRES_PORT", "5432"),\n            "-U", os.environ.get("POSTGRES_USER", "postgres"),\n            "-d", temp_db_name,\n            dump_file\n        ],',
              content
          )
          
          # Modify the database drop command to use environment variables
          content = re.sub(
              r'subprocess\.run\(\["dropdb", db_name\], check=True\)',
              'subprocess.run([\n            "dropdb",\n            "-h", os.environ.get("POSTGRES_HOST", "localhost"),\n            "-p", os.environ.get("POSTGRES_PORT", "5432"),\n            "-U", os.environ.get("POSTGRES_USER", "postgres"),\n            db_name\n        ], check=True, env={**os.environ, "PGPASSWORD": os.environ.get("POSTGRES_PASSWORD", "postgres")})',
              content
          )
          
          # Write the modified content back to the file
          with open('scripts/process_relationships.py', 'w') as f:
              f.write(content)
          EOF
          
          # Run the script to modify the file
          python modify_script.py
          
          # Display the changes for debugging
          echo "Modified process_relationships.py:"
          cat scripts/process_relationships.py | grep -A 5 -B 5 "POSTGRES_HOST"
      
      - name: Process relationships data
        id: process
        if: steps.download.outputs.DOWNLOAD_SUCCESS == 'true'
        env:
          POSTGRES_HOST: localhost
          POSTGRES_PORT: 5432
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
        run: |
          # Get the path of the downloaded file
          DUMP_FILE="${{ steps.download.outputs.DUMP_FILE }}"
          echo "Processing dump file: $DUMP_FILE"
          python scripts/process_relationships.py "$DUMP_FILE" --output data/graph_data.json
      
      - name: Commit and push if data changed
        if: steps.download.outputs.DOWNLOAD_SUCCESS == 'true' && success()
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          
          # Check if graph_data.json exists and has changes
          if [ -f data/graph_data.json ]; then
            git add data/graph_data.json
            
            # Only commit if there are changes
            if git diff --staged --quiet; then
              echo "No changes to graph_data.json"
            else
              git commit -m "Update graph data [skip ci]"
              git push
              echo "Updated graph_data.json committed and pushed"
            fi
          else
            echo "graph_data.json not found"
            exit 1
          fi
      
      - name: Clean up dump file
        if: always()
        run: |
          # Remove any dump files to ensure sensitive data is not kept
          rm -f data/*.dump
          echo "Cleaned up dump files"

